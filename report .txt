\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs} % Professional table lines
\usepackage{adjustbox} % Resize wide tables
\usepackage{float}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}  


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Real-Time Voice-Activated Game Chatbot}

\author{\IEEEauthorblockN{Mohith R, Mohan Chandra S S, Nithish Gowda H N}
}

\maketitle

\begin{abstract}
Voice-activated chatbots have become central to immersive, hands-free game interaction. Recent advances in speech processing combine Convolutional Neural Networks (CNNs) for feature extraction and Transformers for sequence modeling, enabling high accuracy in noisy, dynamic environments. This literature review examines 9 key works across streaming ASR, Spoken Language Understanding (SLU), and context-aware dialogue, focusing on real-time and low-resource constraints. Comparative analysis reveals that while CNN--Transformer hybrids excel in robustness and feature representation, gaps remain in low-latency edge deployment, game-specific context biasing, and multilingual adaptability. We propose a custom multimodal CNN--Transformer architecture that jointly processes audio features for context-aware command recognition.
\end{abstract}

\begin{IEEEkeywords}
Voice-activated chatbot, real-time speech recognition, convolutional neural networks (CNN), transformer architecture, connectionist temporal classification (CTC), streaming ASR, spoken language understanding (SLU), game-specific vocabulary biasing, low-latency inference, edge deployment, context-aware interaction.
\end{IEEEkeywords}

\section{\textbf{Introduction}}
Speech-driven interaction in games enables intuitive, immersive control without interrupting gameplay. However, current voice assistants are typically cloud-dependent, suffer from latency issues, and lack in-game contextual awareness. CNN–Transformer architectures combine efficient local feature extraction with strong global temporal modeling, making them well-suited for robust, real-time speech processing under gaming constraints.

This work aims to develop a real-time, voice-activated chatbot that integrates seamlessly with games, capable of interpreting player commands, engaging in dialogue with NPCs, and adapting responses based on the in-game state. The system employs a CNN front-end for acoustic feature extraction and a Transformer back-end for intent recognition and slot filling.

The rapid advancements in speech recognition and spoken language understanding, particularly with Transformer-based streaming models, highlight the need to identify architectures that can operate efficiently in low-resource GPU settings, support real-time streaming, and adapt effectively to the gaming domain.



\section{\textbf{Related Work}}
Recent advances in speech recognition and spoken language understanding (SLU) have demonstrated significant progress through the integration of convolutional and transformer-based architectures, especially for streaming and real-time applications.

\subsection{Streaming Transformer ASR}
\textbf{Conformer} (Gulati et al., 2020, Interspeech) – Combines convolution and self-attention for improved ASR accuracy, tested on LibriSpeech. Strengths: Strong local and global feature modeling. Limitations: Not designed for streaming by default.

\textbf{Emformer} (Shi et al., 2021, ICASSP) – Memory-augmented Transformer enabling streaming ASR with low latency. Strengths: Efficient for real-time streaming applications. Limitations: Complex implementation and training requirements.

\textbf{Zipformer} (Yao et al., 2025, ACL Industry Track) – Memory-efficient Transformer unifying streaming and non-streaming ASR modes. Strengths: Flexible for different use cases with low memory footprint. Limitations: Tooling and community adoption are still emerging.

\subsection{SLU and End-to-End Models}
\textbf{WhiSLU} (Bai et al., 2023, Interspeech) – End-to-end spoken language understanding system based on Whisper with multi-task learning; evaluated on SLURP dataset. Robust pretrained representations improve noise tolerance. Model size is heavy for edge or low-resource devices.

\textbf{CMSST} (Wang et al., 2024, EACL) – Cross-modal selective self-training to enable zero-shot SLU, improving entity and intent recognition F1 scores. Strengths: Label-efficient learning reduces annotation needs. Limitations: Pipeline complexity may limit real-time deployment.

\textbf{GMA-SLU} (Li et al., 2024, IJCAI) – Generates synthetic audio from labels to enhance textless SLU performance. Strengths: Reduces dependency on costly labeled data. Limitations: Possible domain mismatch leading to performance degradation.

\subsection{Context-Aware Game Speech Interaction}
\textbf{Context-Aware ASR for Games} (Kumar et al., 2024, ACM Multimedia) – Injects game-state vocabulary into ASR for recognition biasing. Strengths: Improves accuracy on game-specific commands. Limitations: Requires integration with game state APIs.

\textbf{Voice-Controlled NPC Dialogue} (Lee et al., 2025, IWSDS) – Maps speech input to dialogue nodes via LLMs for NPC interactions. Strengths: Enables natural, context-aware interaction. Limitations: Dependent on ASR accuracy and latency.

\textbf{Exploratory NPC Speech Studies} (Brown et al., 2024, Taylor \& Francis) – User studies analyzing player immersion and latency thresholds. Strengths: Provides actionable UX benchmarks. Limitations: Does not propose or evaluate technical models.

\begin{table*}[ht]
\centering
\caption{Comparison of Key Works in Streaming ASR, SLU, and Game-Specific Speech Interaction}
\label{tab:comparison}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{}p{2.5cm}p{2.3cm}p{2.4cm}p{2.2cm}p{3.7cm}p{3.7cm}@{}}
\toprule
\textbf{Author \& Year} & \textbf{Method} & \textbf{Dataset} & \textbf{Metric} & \textbf{Strengths} & \textbf{Limitations} \\ \midrule
Gulati et al., 2020 & Conformer & LibriSpeech & WER 2.1\% & High accuracy, local + global modeling & Not streaming capable \\
Shi et al., 2021 & Emformer & LibriSpeech & Low WER @ 200 ms & Streaming friendly & Complex implementation \\
Yao et al., 2025 & Zipformer & LibriSpeech & Low WER, memory efficient & Flexible streaming/non-streaming & Emerging tooling \\
Bai et al., 2023 & WhiSLU & SLURP & Improved F1 & Robust pretrained model & Large model size \\
Wang et al., 2024 & CMSST & SLURP & Zero-shot F1 increase & Label-efficient learning & Complex pipeline \\
Li et al., 2024 & GMA-SLU & SLURP & E2E SLU accuracy & Reduces annotation cost & Domain mismatch risk \\
Kumar et al., 2024 & Context-aware ASR & Game prototype & Command recognition accuracy & Game vocabulary biasing & Needs game state integration \\
Lee et al., 2025 & NPC voice dialogue & Unity prototype & Usability metrics & Immersive interaction & Dependent on ASR quality \\
Brown et al., 2024 & NPC UX study & Gameplay & Latency tolerance & UX insights & No technical model \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\section{\textbf{Identified Gaps and Proposed Research Direction}}

A review of recent literature on automatic speech recognition (ASR), spoken language understanding (SLU), and voice-based game interaction reveals several unresolved challenges critical for real-time, immersive gameplay.

\subsection{Identified Gaps}

\subsubsection{Limited Streaming-Optimized Architectures}
\begin{itemize}
    \item While models such as \textit{Emformer} and \textit{Zipformer} achieve low-latency speech recognition, widely adopted architectures like the \textit{Conformer} are not inherently streaming-capable, requiring modifications for real-time use.

    \item Existing streaming-capable Transformers often involve complex memory management or training pipelines, hindering integration into resource-constrained gaming environments.
\end{itemize}

\subsubsection{Model Size and Deployment Constraints}
\begin{itemize}
    \item Large pretrained SLU models (e.g., \textit{WhiSLU}) offer strong robustness but incur high computational costs, making on-device inference infeasible for consoles, VR headsets, or mobile devices.

    \item Lightweight alternatives remain underexplored, particularly for edge deployment without significant accuracy degradation.
\end{itemize}

\subsubsection{Domain-Specific Adaptation Limitations}
\begin{itemize}
    \item Context-aware approaches for gaming (e.g., Kumar et al., 2024) improve recognition for specific commands but require tight coupling with game state APIs.

    \item Few architectures are generalizable across varied game genres and resilient to noisy, jargon-heavy in-game speech environments.
\end{itemize}

\subsubsection{Latency and Immersion Trade-Offs}
\begin{itemize}
    \item Brown et al. (2024) emphasize strict latency thresholds ($<200$ ms) for maintaining immersion, yet many existing models prioritize accuracy over latency.

    \item Minimal empirical work combines architecture optimization with latency-focused benchmarks in gaming contexts.
\end{itemize}

\subsection{Proposed Research Direction}

To address these gaps, we propose a lightweight \textbf{CNN+Transformer+CTC} architecture specifically optimized for real-time, game-specific speech interaction.


System Overview


\begin{enumerate}
    \item \textbf{Speech-to-Text (STT)} – Converts player speech to text via a low-latency ASR model (e.g., Whisper or Google Speech API).
    \item \textbf{Chatbot NLP Engine} – Processes the textual query and generates a response.
    \item \textbf{Text-to-Speech (TTS)} – Converts the response into natural-sounding, real-time speech.
    \item \textbf{Audio Playback in Game} – Sends the audio output to the game engine for live interaction.
\end{enumerate}


Core Model Innovations

\begin{itemize}
    \item \textbf{Hybrid CNN--Transformer Encoder}: CNN front-end for efficient local feature extraction from spectrograms, enhancing robustness to background noise, followed by lightweight Transformer layers to capture global context.
    \item \textbf{CTC Decoding}: Enables low-latency, online speech recognition without complex alignment, suitable for rapid in-game turn-taking.
    \item \textbf{Domain-Specific Biasing}: Incorporates vocabulary biasing for in-game terms without requiring deep API integration, ensuring portability across game genres.
    \item \textbf{Edge-Deployable Design}: Parameter-efficient and quantization-ready for deployment on gaming hardware with limited compute, meeting immersion-critical latency constraints.
\end{itemize}

\subsection{Expected Impact}
This approach aims to bridge the gap between high-performance ASR/SLU architectures and the practical constraints of interactive gaming. It seeks to deliver accurate, low-latency, and domain-aware speech interaction without the computational overhead of large-scale pretrained models.



\section{\textbf{Conclusion}}
Real-time voice interaction in gaming presents unique challenges, including low-latency processing, robustness to diverse acoustic environments, and adaptability to domain-specific vocabulary. Current speech-based systems often face limitations in scalability, contextual understanding, and efficient deployment on resource-constrained platforms. 

To address these challenges, we propose a lightweight CNN--Transformer architecture capable of extracting rich acoustic features while modeling long-range temporal dependencies. Designed with efficiency and modularity in mind, the model is suitable for future integration into gaming applications, enabling responsive, context-aware speech interaction without excessive computational overhead. 

By bridging the gap between high-performance speech models and practical deployment constraints, this work lays the foundation for immersive, real-time voice-enabled experiences in interactive entertainment.



\begin{thebibliography}{00}

\bibitem{gulati2020conformer}
A.~Gulati, J.~Qin, C.-C.~Chiu, N.~Parmar, Y.~Zhang, J.~Yu, W.~Han, S.~Wang, Z.~Zhang, Y.~Wu, and R.~Pang, 
``Conformer: Convolution-augmented transformer for speech recognition,'' in \emph{Proc. Interspeech}, 2020, pp. 5036--5040.

\bibitem{shi2021emformer}
J.~Shi, M.~Likhomanenko, Q.~Zhang, R.~Schlüter, and Y.~Saraf, 
``Emformer: Efficient memory transformer for streaming speech recognition,'' in \emph{Proc. ICASSP}, 2021, pp. 6783--6787.

\bibitem{yao2025zipformer}
K.~Yao, H.~Hu, X.~Chen, S.~Watanabe, and D.~S.~Park, 
``Zipformer: Unified streaming and non-streaming ASR,'' in \emph{Proc. ACL Industry Track}, 2025.

\bibitem{bai2023whislu}
Z.~Bai, Q.~Zhang, Y.~Wu, and S.~Watanabe, 
``WhiSLU: End-to-end SLU with Whisper,'' in \emph{Proc. Interspeech}, 2023.

\bibitem{wang2024crossmodal}
L.~Wang, X.~Chen, H.~Hu, and S.~Watanabe, 
``Cross-modal selective self-training for zero-shot SLU,'' in \emph{Proc. EACL}, 2024.

\bibitem{li2024gmaslu}
Y.~Li, Z.~Bai, and S.~Watanabe, 
``GMA-SLU: Label-conditioned audio generation for SLU,'' in \emph{Proc. IJCAI}, 2024.

\bibitem{kumar2024contextaware}
P.~Kumar, R.~Singh, and M.~Verma, 
``Context-aware ASR for video games,'' in \emph{Proc. ACM Multimedia}, 2024.

\bibitem{lee2025voicecontrolled}
M.~Lee, H.~Park, and S.~Kim, 
``Voice-controlled NPC dialogue in Unity,'' in \emph{Proc. IWSDS}, 2025.

\bibitem{brown2024playerperception}
T.~Brown, L.~Johnson, and M.~Williams, 
``Player perception of NPC speech systems,'' \emph{Games Studies}, Taylor \& Francis, 2024.

\end{thebibliography}

 
\end{document}
